---
title: "Bilingualism Results"
author: "Sophie Harte"
date: "02/11/2020"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(tidyverse)
require(flextable)
require(ggplot2)
```

```{r read_data, echo=FALSE, messgae = FALSE}
# read French/German data in
L1_data <- read.csv('Results_L1.csv')
L2_data <- read.csv('Results_L2.csv')
background_data_total <- read.csv('background_data.csv', stringsAsFactors = FALSE)
colnames(background_data_total)[1] <-  colnames(background_data_total)[1] <- 'ID'

# read Japanese data in 
Miho_L1_data <- read.csv('Miho_Results_L1.csv')
Miho_L2_data <- read.csv('Miho_Results_L2.csv')
Miho_background_data_total <- read.csv('Miho_background_data.csv', stringsAsFactors = FALSE)
colnames(Miho_background_data_total)[1] <-  colnames(Miho_background_data_total)[1] <- 'ID'
Miho_background_data_total$ID <- as.numeric(gsub("BL", replacement = "", Miho_background_data_total$ID))

```

# Study 1: French/German Speakers

## Methods

### Participants

```{r study1_participants, echo=FALSE, message=FALSE}
# How many participants are there, and how many were excluded from the analysis?
nsubj_all <- dim(L1_data)[1]
# If a subject has data excluded in EITHER language, we'll have to exclude them.
exclude <- L1_data$L1.exclusions + L2_data$L2.exclusions
# We will include the rows in L1_data and L2_data for any participants where exclude = 0
L1_data <- L1_data[which(exclude==0), ]
L2_data <- L2_data[which(exclude==0), ]
# Remove excluded participants from backgound data too
background_data <- background_data_total[which(exclude==0), ]
nsubj <- dim(L1_data)[1]
background_data <- background_data %>% mutate(L2_prof = (L2_prof_Speaking+L2_prof_Reading+L2_prof_Understanding)/3)

# Calculate demographics summary statistics
demo_summary <- background_data %>% 
  summarise(N_total = n(), N_german = length(which(L1==1)), N_french = length(which(L1==2)),
            N_female = length(which(Gender==0)), N_male = length(which(Gender==1)),
            mean_age = mean(Age), sd_age = sd(Age),
            mean_EHI = mean(EHI), sd_EHI = sd(EHI), min_EHI = min(EHI), max_EHI = max(EHI),
            languages = mean(NoL), sd_languages = sd(NoL),
            mean_aoa = mean(L2_Acquisition), sd_aoa = sd(L2_Acquisition),
            mean_aoF = mean(L2_Fluent), sd_aoF = sd(L2_Fluent),
            mean_accent = mean(L2_exp_Accent), sd_accent = sd(L2_exp_Accent),
            mean_read = mean(L2_prof_Reading), sd_read = sd(L2_prof_Reading),
            mean_und = mean(L2_prof_Understanding), sd_und = sd(L2_prof_Understanding),
            mean_speak = mean(L2_prof_Speaking), sd_speak = sd(L2_prof_Speaking),
            mean_prof = mean(L2_prof), sd_prof = sd(L2_prof))

```

Participants were recruited through the Oxford University German Society and Oxford University French Society, as well as through posters in the Experimental Psychology building. Participants were aged over 18 years and were either German-English or French-English bilinguals. All had normal or corrected to normal vision. Individuals with a diagnosis of any speech, language or learning impairment, affected by a neurological disorder or taking medication affecting brain function e.g. antidepressants, were not included in the study. 

A total of 40 individuals were assessed for viability as study participants. In total, 14 participants were excluded for a range of reasons, including no suitable Doppler signal, due to the inability to find a suitable temporal window in the skull, or failure to stabilize the Doppler signal for the required amount of time (11 participants), or low quality data (3 participants). Data was collected from `r nsubj_all` participants. During the analysis, data from `r length(which(exclude!=0))` participants was excluded due to poor data quality or an insufficient number of useable trials. All further analyses are based on the final sample of `r demo_summary$N_total` participants (`r demo_summary$N_female` female; mean age = `r round(demo_summary$mean_age, 2)` years, sd = `r round(demo_summary$sd_age, 2)` years).

### Ethics Statement
The study was approved by the University of Oxford Central Research Ethics Committee (CUREC), approval number, MS-IDREC-C1-2015-126). All participants provided written informed consent.

### Handedness
Handedness was assessed via the Edinburgh Handedness Inventory (EHI; Oldfield, 1971). The inventory consists of 10 items assessing dominance of a person’s right or left hand in everyday activities. Each item is scored on a 5 step scale (“always left”, “usually left”, “both equally”, “usually right”, “always right”). A person can score between -100 and +100 for each item and an overall score is calculated by averaging across all items (“always left” -100; “usually left” -50; “both equally” 0).

### Language History
The Language Experience and Proficiency Questionnaire (LEAP-Q; Marian et al., 2007) was used to assess language history for all participants. The LEAP-Q is a self-assessment questionnaire consisting of nine general questions and seven additional questions per language that explore acquisition history, context of acquisition, present language use, and language preference and proficiency ratings across language domains (speaking, understanding and reading) as well as accent ratings. 

Of relevance for our analyses were the numbers of languages spoken; age of acquisition of English; age of acheiving fluency in English; self-reported strength of foreign accent when speaking English (on a scale from 0 [none] to 10 [pervasive]); and self-reported proficiency ratings in speaking, reading and understanding English (on a scale from 0 [none] to 10 [perfect]). An overall proficiency rating was calculated by taking the mean ratings for proficiency in speaking, reading and understanding English.

### fTCD Apparatus
A commercially available transcranial Doppler ultrasonography device (DWL, Multidop T2; manufacturer, DWL Elektronische Systeme, Singen, Germany) was used for continuous measurements of the changes in cerebral blood flow velocity (CBFV) through the left and right MCA. The MCA was insonated at ~5 cm (40–60 mm). Activity in frontal and medial cortical areas, supplied by the anterior cerebral artery, and inferior temporal cortex, supplied by the posterior cerebral artery, do not contribute to the measurements made in the MCA. Two 2-MHz transducer probes, which are relatively insensitive to participant motion, were mounted on a screw-top headset and positioned bilaterally over the temporal skull window (Deppe et al., 2004).

### Word Generation Task
Tasks were programmed using Presentation® software (version 17.2; www.neurobs.com). All instructions were presented centrally in white Arial font on a black background. Each participant was tested in English (L2) and their native language (L1; French or German) in a single session using two tasks, each consisting of 23 trials.

The order of the two languages was counterbalanced across participants and the entire testing session lasted between 75 and 90 minutes. The experimenter spoke English at all times. So that they were focussed on their native language, participants were asked to describe the Cookie Theft picture of the Boston Diagnostic Aphasia Examination in their native language prior to being tested in that language (Goodglass & Kaplan, 1983).

The cued word generation paradigms were based on Knecht and colleagues’ 1998 paradigm (Knecht et al., 1998b). For each trial, the participant is shown a letter and is asked to silently generate words starting with that letter. Each task comprised 23 trials and lasted for around 20 minutes. We excluded the three letters with the lowest first letter word frequency: Q, X and Y in English; Q, X and Z in German; and W, X and Y in French. Task instructions for the German and French word generation tasks were translated into German and French by the experimenter.

Each trial started with an auditory tone and the written instruction “Clear Mind” (5s), followed by the letter cue to which the participant silently generated words (15s), and then overt word generation (5s) (Figure 1). To restore baseline activity, participants were instructed to relax (25s) at the end of each trial. Event markers were sent to the Multi-Dop system when the letter cue appeared, denoting trial onset for subsequent analysis of the Doppler signal.

### fTCD Analysis

The CBFV data were analysed using custom scripts in R Studio (R Studio Team, 2015), which are available on Open Science Framework (link). The data preprocessing followed conventional methods (Deppe, Ringelstein & Knecht, 2004), and included the following steps:

-	Downsampling from 100Hz to 25Hz

-	Epoching from -11s to 21s relative to the onset of the ‘Clear Mind’ cue

-	Manual exclusion of trials with obvious spiking or dropout artefacts

-	Automated detection of data points with signal intensity beyond 0.0001-0.999 quantiles. If a trial contained one of these extreme data points, it was replaced by the mean for that epoch; if it contained more than one, the trial was excluded from further analysis

-	Normalisation of signal intensity by dividing CBFV values by the mean for all included trials and multiplying by 100

-	Heart cycle integration by averaging the signal intensity from peak to peak of the heart beat

-	Baseline correction by subtracting the mean CBFV across the baseline period (-10s to 0s relative to the ‘Clear Mind’ cue) from all values in the trial

-	Automated detection and rejection of trials containing normalized values below 60 or 140.

Participants with less than 15 useable trials for either language were excluded from all further analyses. For each participant that was included in the analysis, a grand mean was calculated over all of their included trials. A laterality index (LI) was calculated by taking the mean of the difference between left and right CBFVs (L-R) within a period of interest (POI) that started 8s after the ‘Clear Mind’ cue (i.e. 3s after the word generation task had begun) and ended at 20s (i.e. when the covert generation task ended). The start time of the POI was chosen to allow time for the blood flow to respond to the task; and the end time was chosen to prevent capturing the response to the overt speech generation phase.

This method of calculating LI using the mean L-R difference across the whole of the POI (the ‘mean’ method) deviates from the conventional method of using the mean of a narrow time window around the peak difference within the POI (the ‘peak’ method). It has been shown that it is unusual for the peak method to result in an LI of zero, which distorts the measurement of laterality and results in less normally distributed LI values (Woodhead et al., 2020). Nonetheless, peak LI values are available in the online data repository if readers wish to see them.

Finally, we identified and excluded datasets with unusually high trial-by-trial variability using the Hoaglin and Iglewicz (1987) outlier detection method. For this analysis, LI was calculated for each trial, rather than just for the grand average. The standard error of the LI values was then calculated. Outliers were  defined as datasets where the standard error was above an upper threshold, calculated as:

Upper threshold = Q3 + 2.2 * (Q3 – Q1)

whereby Q1 is the first quantile of the standard errors among all participants, and Q3 is the third quartile. Participants who had standard error above the upper threshold for either L1 or L2 were excluded from all further analyses.


### Statistical Analysis
To test our main hypothesis, the association between strength of lateralization (LI values) for L1 and L2 was tested using Spearman’s correlation and visualized using a scatterplot. For our second hypothesis, we tested whether there was a correlation between LI value for L2 and age of acquisition of L2. 

In addition, we tested the correlation between LI values and strength of handedness (EHI quotient). The impact of testing order (L1 then L2, or L2 then L1) was assessed using two independent samples t-tests: (1) comparing LI values for L1 in participants tested with it first, versus those tested with it second; and (2) the same comparison, but for LI values for L2.


```{r study1_descriptive_statistics, echo=FALSE, message=FALSE, warning=FALSE}
# Calculate mean and SEM LI value for L1 and L2
L1_summary_stats <- L1_data %>% summarise(L1_mean = mean(L1.mean_LI), L1_se = sd(L1.mean_LI)/sqrt(nsubj),
                                          L1_mean_trials = mean(L1.N), L1_min_trials = min(L1.N), L1_max_trials = max(L1.N)) 
L2_summary_stats <- L2_data %>% summarise(L2_mean = mean(L2.mean_LI), L2_se = sd(L2.mean_LI)/sqrt(nsubj),
                                          L2_mean_trials = mean(L2.N), L2_min_trials = min(L2.N), L2_max_trials = max(L2.N)) 

# Number of Ps left/ bilaterally lateralised
L1_laterality <- L1_data %>% group_by(L1.mean_laterality) %>% summarise(N=n(), pc=N/nsubj*100)
L2_laterality <- L2_data %>% group_by(L2.mean_laterality) %>% summarise(N=n(), pc=N/nsubj*100)


# Make a massive table
summary_stats <- matrix(data = NA, nrow = 2, ncol = 7)
colnames(summary_stats) <- c('Language','Mean trials', 'mean LI', 'se LI', '% left', '% bilateral', '% right')
summary_stats[, 1] <- c('L1', 'L2')
summary_stats[, 2] <- round(c(L1_summary_stats$L1_mean_trials, L2_summary_stats$L2_mean_trials), 2)
summary_stats[, 3] <-  round(c(L1_summary_stats$L1_mean, L2_summary_stats$L2_mean), 2)
summary_stats[, 4] <-  round(c(L1_summary_stats$L1_se, L2_summary_stats$L2_se), 2)
summary_stats[, 5] <- round(c(L1_laterality$pc[2], L2_laterality$pc[2]))
summary_stats[, 6] <- round(c(L1_laterality$pc[1], L2_laterality$pc[1]))
summary_stats[, 7] <- round(c(L1_laterality$pc[3], L2_laterality$pc[3]))
summary_stats[which(is.na(summary_stats[,7])), 7] <- 0 
summary_stats <- as.data.frame(summary_stats)

# Scatterplot of L1 and L2 data
plotdata <- inner_join(x = L1_data,
                        y = L2_data)

# Correlation between L1 and L2
L1_L2_corr <- cor.test (plotdata$L1.mean_LI, plotdata$L2.mean_LI, 
                        method = "spearman", 
                        exact = FALSE)

Fig2 <- ggplot(data = plotdata, aes(x = L1.mean_LI, y = L2.mean_LI)) + 
  geom_point()+
  theme_bw()+
  xlab("LI for L1 (French / German)") +  
  ylab("LI for L2 (English)") + 
  xlim(-3,9) + ylim (-3,9) + 
  geom_errorbar(aes(ymin = L2.mean_LI - L2.mean_se, ymax = L2.mean_LI + L1.mean_se),linetype = "dotted") + 
  geom_errorbarh(aes(xmin = L1.mean_LI - L1.mean_se, xmax = L1.mean_LI + L1.mean_se), linetype = "dotted") +
  geom_text(x = 1, y = 7, label = paste('r = ', round(L1_L2_corr$estimate, 3)), colour = 'red') +
  ggtitle('Figure 2')

# Check correlations with EHI
EHI_LI_L1_cor <- cor.test (background_data$EHI, L1_data$L1.mean_LI, method = "spearman")
EHI_LI_L2_cor <- cor.test (background_data$EHI, L2_data$L2.mean_LI, method = "spearman")

# Check for effect of testing order (0 = L2 tested first, 1 = L1 tested first)
total_info <- inner_join(x = plotdata, y = background_data, by = c("ID"))
nL1_testedfirst <- nrow(background_data[background_data$Tested == 1,]) #how many tested with L1 first
nL1_testedsecond <- nrow(background_data[background_data$Tested == 0,]) # how many tested with L2 first
testing_0 <- filter(total_info, Tested == "0")
testing_1 <- filter(total_info, Tested == "1")
testing_ttest_L1 <- t.test(testing_0$L1.mean_LI, testing_1$L1.mean_LI) # testing if there is a sig. diff between laterality scoes of L1 in those tested in their native language first or second.
testing_ttest_L2 <- t.test(testing_0$L2.mean_LI, testing_1$L2.mean_LI) # testing if there is a sig. diff between laterality scoes of L2 in those tested in their native language first or second.
```
## Results

### Handedness

Summary statistics for the EHI handedness measure can be seen in Table 1. All participants but one had EHI values above 0, indicating right handedness. The remaining participant had an EHI of -20, indicating weak left handedness. There were no correlations between laterality strength and handedness scores on the EHI, for either L1 (r = `r round(EHI_LI_L1_cor$estimate, 3)`) or L2 (r = `r round(EHI_LI_L2_cor$estimate, 3)`).


```{r table1, echo=FALSE, warning=FALSE}
# Make a demographics table
demo_table <- matrix(data=NA, nrow=10, ncol=2)
colnames(demo_table) <- c('Characteristic', 'Mean (sd)')
demo_table[,1] <- c('Age, years', 'EHI, /100', 'Languages spoken',
                          'Age of acquisition, years', 'Age of fluency, years', 'Accent, /10',
                          'Proficiency: speaking, /10', 'Proficiency: reading, /10', 
                          'Proficiency: understanding, /10', 'Proficiency: total, /10')
#demo_table[1,2] <- paste0(demo_summary$N_total, ' (', demo_summary$N_male, ':', demo_summary$N_female, ')')
demo_table[1,2] <- paste0(round(demo_summary$mean_age, 2), ' (', round(demo_summary$sd_age, 2), ')')
demo_table[2,2] <- paste0(round(demo_summary$mean_EHI, 2), ' (', round(demo_summary$sd_EHI, 2), ')')
demo_table[3,2] <- paste0(round(demo_summary$languages, 2), ' (', round(demo_summary$sd_languages, 2), ')')
demo_table[4,2] <- paste0(round(demo_summary$mean_aoa, 2), ' (', round(demo_summary$sd_aoa, 2), ')')
demo_table[5,2] <- paste0(round(demo_summary$mean_aoF, 2), ' (', round(demo_summary$sd_aoF, 2), ')')
demo_table[6,2] <- paste0(round(demo_summary$mean_accent, 2), ' (', round(demo_summary$sd_accent, 2), ')')
demo_table[7,2] <- paste0(round(demo_summary$mean_speak, 2), ' (', round(demo_summary$sd_speak, 2), ')')
demo_table[8,2] <- paste0(round(demo_summary$mean_read, 2), ' (', round(demo_summary$sd_read, 2), ')')
demo_table[9,2] <- paste0(round(demo_summary$mean_und, 2), ' (', round(demo_summary$sd_und, 2), ')')
demo_table[10,2] <- paste0(round(demo_summary$mean_prof, 2), ' (', round(demo_summary$sd_prof, 2), ')')
demo_table <- as.data.frame(demo_table)

table1 <- flextable(demo_table) %>% set_caption(table1, caption=paste0('Table 1. Demographics for the Study 1 participants, N=',nsubj, ' (', demo_summary$N_female, ' female)')) 
table1 <- autofit(table1)
table1
```

### Language History

Summary statistics for the language history questionnaire can be seen in Table 1. Self-reported proficiency in speaking, reading and understanding English were all generally high (all around 9/10), with a minimum for any individual rating of 6/10. Age of acquisition was more variable, ranging from `r min(background_data$L2_Acquisition)` to `r max(background_data$L2_Acquisition)`.

### fTCD Data Quality and Reliability

```{r study1_data_quality, echo=FALSE, warning = FALSE}
# Report number of excluded trials
excluded_trials_L1 <- 100 - round(sum(L1_data$L1.N) / sum(L1_data$L1.nMark) * 100, 2)
excluded_trials_L2 <- 100 - round(sum(L2_data$L2.N) / sum(L2_data$L2.nMark) * 100, 2)

# Here we will use the Shapiro Wilks normality test to see if the LI values for L1 and L2 are normally distributed
L1_normality <- shapiro.test(L1_data$L1.mean_LI)
L2_normality <- shapiro.test(L2_data$L2.mean_LI)

# spearman correlation test on odds and evens - means: Grabitz data 
L1_splithalf <- cor.test(L1_data$L1.mean_odd, L1_data$L1.mean_even, method = "spearman")
L2_splithalf <- cor.test(L2_data$L2.mean_odd, L2_data$L2.mean_even, method = "spearman")  
```

  
  As mentioned in the Methods, `r length(which(exclude!=0))` participants were excluded from the analysis due to poor data quality or an insufficient number of trials. For the remaining participants, `r excluded_trials_L1`% of trials were excluded for L1, and `r excluded_trials_L2`% for L2.
  
  Normality of the LI values was assessed using Shapiro-Wilk tests. For L1 the data significantly deviated from normality (W = `r round(L1_normality$statistic, 2)`, p = `r round(L1_normality$p.value, 3)`), but for L2 it did not (W = `r round(L2_normality$statistic, 2)`, p = `r round(L2_normality$p.value, 3)`).

  Split-half reliability was assessed by correlating the LI values from odd and even trials. The spearman's correlation for the L1 data was `r round(L1_splithalf$estimate, 2)`, and for the L2 data it was `r round(L2_splithalf$estimate, 2)`, indicating medium to good reliability.
  

### LI Values

```{r Table2, warning=FALSE, message=FALSE, echo=FALSE}
table2 <- flextable(summary_stats) %>% set_caption(table2, caption='Table 2. Summary statistics for Study 1 laterality indices')
table2 <- autofit(table2)
table2

```

  Table 2 shows summary statistics for the LI values for L1 and L2. The percentage of participants in each group categorised as left lateralised, bilateral or right lateralised is also shown. The majority of participants were left lateralised, with only around 10% showing bilateral activation. No participants showed right lateralisation for either L1 or L2. T-tests showed that there were no significant effects of testing order on LI values, either for L1 (p = `r round(testing_ttest_L1$p.value, 3)`) or L2 (p = `r round(testing_ttest_L2$p.value, 3)`).
  
  As can be seen in the scatterplot in Figure 2, laterality indices for L1 and L2 were very similar, and were strongly correlated (Spearman's R = `r round(L1_L2_corr$estimate, 3)`). 

```{r Fig2, warning=FALSE, message=FALSE, echo=FALSE}

print(Fig2)

```

### Correlations Between LI and Age of Acquisition

```{r study1_aoa, echo=FALSE, messgae = FALSE, warning=FALSE}

# Correlation between AOA and LIs
aoa_L2_corr <- cor.test(total_info$L2_Acquisition, total_info$L2.mean_LI, method = "spearman")
aoF_L2_corr <- cor.test(total_info$L2_Fluent, total_info$L2.mean_LI, method = "spearman")
prof_L2_corr <- cor.test(total_info$L2_prof, total_info$L2.mean_LI, method="spearman")

```
To test whether strength of laterality is affected by the age of acquisition, we computed Spearman's correlations between the LI values for L2 (English) and the age of acquisition of English. This was not significant (r = `r round(aoa_L2_corr$estimate, 2)`, p = `r round(aoa_L2_corr$p.value, 3)`). Similarly, there were no significant correlations between LI values for L2 and the age of acheiving fluency in English (r = `r round(aoF_L2_corr$estimate, 2)`, p = `r round(aoF_L2_corr$p.value, 3)`) or overall proficiency in English (r = `r round(prof_L2_corr$estimate, 2)`, p = `r round(prof_L2_corr$p.value, 3)`).

## Discussion

TO BE COMPLETED

# Study 2: Japanese Speakers

## Methods

### Participants

```{r study2_participants, echo=FALSE, warning=FALSE, message=FALSE}
# How many participants are there, and how many were excluded from the analysis?
Miho_nsubj_all <- dim(Miho_L1_data)[1]

# If a subject has data excluded in EITHER language, we'll have to exclude them
# Phonological and Semantic fluency tasks will be treated separately
Phon_exclude <- Miho_L1_data$L1_Phon.exclusions + Miho_L2_data$L2_Phon.exclusions
Sem_exclude <- Miho_L1_data$L1_Sem.exclusions + Miho_L2_data$L2_Sem.exclusions 
Miho_exclude <- Phon_exclude + Sem_exclude

# We will include the rows in L1_data and L2_data for any participants where exclude = 0
L1_Phon_data <- Miho_L1_data[which(Miho_exclude==0), ]
L2_Phon_data <- Miho_L2_data[which(Miho_exclude==0), ]
L1_Sem_data <- Miho_L1_data[which(Miho_exclude==0), ]
L2_Sem_data <- Miho_L2_data[which(Miho_exclude==0), ]

# How many subjects were included for each task?
Phon_nsubj <- dim(L1_Phon_data)[1]
Sem_nsubj <- dim(L1_Sem_data)[1]
# Remove excluded participants from backgound data too
Miho_background_data <- Miho_background_data_total[which(Miho_exclude==0), ]

# Calculate demographics summary statistics
#NEED TO CHECK 0 IS DEFINITLEY FEMALE!!

miho_demo_summary <- Miho_background_data %>%
  summarise(N_total = n(), N_female_j = length(which(Gender == "0")), N_male_j = length(which(Gender== "1")),
            mean_age_j = mean(Age, na.rm = TRUE), sd_age = sd(Age, na.rm = TRUE), 
            mean_aoa_L2_j = mean(aoa_L2, na.rm = TRUE), sd_aoa_L2_j = sd(aoa_L2, na.rm = TRUE),
            mean_eng_score_j = mean(Eng_score, na.rm = TRUE), sd_eng_score_j = sd(Eng_score, na.rm = TRUE),
            mean_eng_use_month_j = mean(length_useEng_months, na.rm = TRUE), sd_eng_use_month_j = sd(length_useEng_months, na.rm = TRUE),
            mean_eng_speak_j = mean(Eng_Speak, na.rm = TRUE), sd_eng_speak_j = sd(Eng_Speak, na.rm = TRUE),
            mean_eng_listen_j = mean(Eng_Listen, na.rm = TRUE), sd_eng_listen_j = sd(Eng_Listen, na.rm = TRUE),
            mean_eng_read_j = mean(Eng_Read, na.rm = TRUE), sd_eng_read_j = sd(Eng_Read, na.rm = TRUE), 
            mean_eng_write_j = mean(Eng_Write, na.rm = TRUE), sd_eng_write_j = sd(Eng_Write, na.rm = TRUE),
            )

```

### Ethics Statement

### Language History

### fTCD Apparatus

### Word Generation Task

### fTCD Analysis

```{r study2_descriptive_statistics, echo=FALSE, warning=FALSE, message=FALSE}
# Calculate mean and SEM LI value for L1 and L2: Phonological fluency
L1_Phon_summary_stats <-
  L1_Phon_data %>% summarise(L1_mean = mean(L1_Phon.mean_LI), L1_se = sd(L1_Phon.mean_LI)/sqrt(Phon_nsubj),
                                  L1_mean_trials = mean(L1_Phon.N), L1_min_trials = min(L1_Phon.N),
                                  L1_max_trials = max(L1_Phon.N))

L2_Phon_summary_stats <-
  L2_Phon_data %>% summarise(L2_mean = mean(L2_Phon.mean_LI), L2_se = sd(L2_Phon.mean_LI)/sqrt(Phon_nsubj),
                                  L2_mean_trials = mean(L2_Phon.N), L2_min_trials = min(L2_Phon.N),
                                  L2_max_trials = max(L2_Phon.N))

# The mean and SEM LI value for L1 and L2: Semantic fluency
L1_Sem_summary_stats <-
  L1_Sem_data %>% summarise(L1_mean = mean(L1_Sem.mean_LI), L1_se = sd(L1_Sem.mean_LI)/sqrt(Sem_nsubj),
                                  L1_mean_trials = mean(L1_Sem.N), L1_min_trials = min(L1_Sem.N),
                                  L1_max_trials = max(L1_Sem.N))

L2_Sem_summary_stats <-
  L2_Sem_data %>% summarise(L2_mean = mean(L2_Sem.mean_LI), L2_se = sd(L2_Sem.mean_LI)/sqrt(Sem_nsubj),
                                  L2_mean_trials = mean(L2_Sem.N), L2_min_trials = min(L2_Sem.N),
                                  L2_max_trials = max(L2_Sem.N))

# Number of Ps left/ bilaterally lateralised for L1/L2, Phon/Sem fluency
L1_Phon_laterality <- L1_Phon_data %>%
  group_by(L1_Phon.mean_laterality) %>% summarise(N=n(), pc=N/Phon_nsubj*100)
L2_Phon_laterality <- L2_Phon_data %>%
  group_by(L2_Phon.mean_laterality) %>% summarise(N=n(), pc=N/Phon_nsubj*100)
L1_Sem_laterality <- L1_Sem_data %>%
  group_by(L1_Sem.mean_laterality) %>% summarise(N=n(), pc=N/Sem_nsubj*100)
L2_Sem_laterality <- L2_Sem_data %>%
  group_by(L2_Sem.mean_laterality) %>% summarise(N=n(), pc=N/Sem_nsubj*100)

# Make a summary table
summary_stats <- matrix(data = NA, nrow = 4, ncol = 8)
colnames(summary_stats) <- c('Task','Language','Mean trials', 'mean LI', 'se LI', '% left', '% bilateral', '% right')
summary_stats[, 1] <- c('Phonological','Phonological','Semantic','Semantic')
summary_stats[, 2] <- c('L1','L2','L1','L2')
summary_stats[, 3] <- round(c(L1_Phon_summary_stats$L1_mean_trials, L2_Phon_summary_stats$L2_mean_trials,
                              L1_Sem_summary_stats$L1_mean_trials, L2_Sem_summary_stats$L2_mean_trials), 2)
summary_stats[, 4] <-  round(c(L1_Phon_summary_stats$L1_mean, L2_Phon_summary_stats$L2_mean,
                               L1_Sem_summary_stats$L1_mean, L2_Sem_summary_stats$L2_mean), 2)
summary_stats[, 5] <-  round(c(L1_Phon_summary_stats$L1_se, L2_Phon_summary_stats$L2_se,
                               L1_Sem_summary_stats$L1_se, L2_Sem_summary_stats$L2_se), 2)
summary_stats[, 6] <- round(c(L1_Phon_laterality$pc[2], L2_Phon_laterality$pc[2],
                              L1_Sem_laterality$pc[2], L2_Sem_laterality$pc[2]))
summary_stats[, 7] <- round(c(L1_Phon_laterality$pc[1], L2_Phon_laterality$pc[1],
                              L1_Sem_laterality$pc[1], L2_Sem_laterality$pc[1]))
summary_stats[, 8] <- round(c(L1_Phon_laterality$pc[3], L2_Phon_laterality$pc[3],
                              L1_Sem_laterality$pc[3], L2_Sem_laterality$pc[3]))
summary_stats[which(is.na(summary_stats[,8])), 8] <- 0 
summary_stats <- as.data.frame(summary_stats)

# Plot Miho Phon L1 and L2 data
plotdata_j <- inner_join(x = L1_Phon_data, L2_Phon_data)
L1_L2_Phon_corr <- cor.test (plotdata_j$L1_Phon.mean_LI, plotdata_j$L2_Phon.mean_LI, method = "spearman")

Fig3 <- ggplot(data = plotdata_j, aes(x = L1_Phon.mean_LI, y = L2_Phon.mean_LI)) + 
  geom_point()+
  theme_bw()+
  xlab("LI for L1 (Japanese), Phonological Fluency") +  
  ylab("LI for L2 (English), Phonological Fluency") + 
  xlim(-3,9) + ylim (-3,9) + 
  geom_errorbar(aes(ymin = L2_Phon.mean_LI - L2_Phon.mean_se, ymax = L2_Phon.mean_LI + L1_Phon.mean_se),linetype = "dotted") + 
  geom_errorbarh(aes(xmin = L1_Phon.mean_LI - L1_Phon.mean_se, xmax = L1_Phon.mean_LI + L1_Phon.mean_se), linetype = "dotted")+
  geom_text(x = 1, y = 7, label = paste('r = ', round(L1_L2_corr$estimate, 3)), colour = 'red') +
  ggtitle('Figure 3')

# Plot Miho Sem L1 and L2 data
plotdata_j <- inner_join(x = L1_Sem_data, y = L2_Sem_data)
L1_L2_corr <- cor.test (plotdata_j$L1_Sem.mean_LI, plotdata_j$L2_Sem.mean_LI, method = "spearman")

Fig4 <- ggplot(data = plotdata_j, aes(x = L1_Sem.mean_LI, y = L2_Sem.mean_LI)) +
  geom_point()+
  theme_bw()+
  xlab("LI for L1 (Japanese), Semantic Fluency") +
  ylab("LI for L2 (English), Semantic Fluency") +
  xlim(0-3,9) + ylim (-3,9) +
  geom_errorbar(aes(ymin = L2_Sem.mean_LI - L2_Sem.mean_se, ymax = L2_Sem.mean_LI + L1_Sem.mean_se),linetype = "dotted") +
  geom_errorbarh(aes(xmin = L1_Sem.mean_LI - L1_Sem.mean_se, xmax = L1_Sem.mean_LI + L1_Sem.mean_se), linetype = "dotted")+
  geom_text(x = 1, y = 7, label = paste('r = ', round(L1_L2_corr$estimate, 3)), colour = 'red') +
  ggtitle('Figure 4')

# Check correlations with EHI
#TO COMPLETE
# EHI_LI_L1_Phon_cor <- cor.test (background_data$EHI, L1_data$L1_Phon.mean_LI, method = "spearman")
# EHI_LI_L2_Phon_cor <- cor.test (background_data$EHI, L2_data$L2_Phon.mean_LI, method = "spearman")
# EHI_LI_L1_Sem_cor <- cor.test (background_data$EHI, L1_data$L1_Sem.mean_LI, method = "spearman")
# EHI_LI_L2_Sem_cor <- cor.test (background_data$EHI, L2_data$L2_Sem.mean_LI, method = "spearman")

# Check for effect of testing order (0 = L2 tested first, 1 = L1 tested first)
# TO COMPLETE
# total_info <- inner_join(x = plotdata_j, y = background_data, by = c("ID"))
# nL1_testedfirst <- nrow(background_data[background_data$Tested == 1,]) #how many tested with L1 first
# nL1_testedsecond <- nrow(background_data[background_data$Tested == 0,]) # how many tested with L2 first
# testing_0 <- filter(total_info, Tested == "0")
# testing_1 <- filter(total_info, Tested == "1")
# testing_ttest_L1 <- t.test(testing_0$L1.mean_LI, testing_1$L1.mean_LI) # testing if there is a sig. diff between laterality scoes of L1 in those tested in their native language first or second.
# testing_ttest_L2 <- t.test(testing_0$L2.mean_LI, testing_1$L2.mean_LI) # testing if there is a sig. diff between laterality scoes of L2 in those tested in their native language first or second.
```

## Results

```{r table3, echo=FALSE, warning=FALSE, message=FALSE}

#### demographics table ####
# Update names of rows when we get more info on background demographics. 
# Make a demographics table

############

demo_table_j <- matrix(data=NA, nrow=8, ncol=2)
colnames(demo_table_j) <- c('Characteristic', 'Mean (sd)')

demo_table_j[,1] <- c('Age, years', 'Age of English Acquisition, years', 'mean English score', 'Use of English, months', 'English speaking', 'English listening', 'English Reading', 'English Writing')

demo_table_j[1,2] <- paste0(round(miho_demo_summary$mean_age_j, 2), ' (', round(miho_demo_summary$sd_age, 2), ')')
demo_table_j[2,2] <- paste0(round(miho_demo_summary$mean_aoa_L2_j, 2), ' (', round(miho_demo_summary$sd_aoa_L2_j, 2), ')')
demo_table_j[3,2] <- paste0(round(miho_demo_summary$mean_eng_score_j, 2), ' (', round(miho_demo_summary$sd_eng_score_j, 2), ')')
demo_table_j[4,2] <- paste0(round(miho_demo_summary$mean_eng_use_month_j, 2), ' (', round(miho_demo_summary$sd_eng_use_month_j, 2), ')')
demo_table_j[5,2] <- paste0(round(miho_demo_summary$mean_eng_speak_j, 2), ' (', round(miho_demo_summary$sd_eng_speak_j, 2), ')')
demo_table_j[6,2] <- paste0(round(miho_demo_summary$mean_eng_listen_j, 2), ' (', round(miho_demo_summary$sd_eng_listen_j, 2), ')')
demo_table_j[7,2] <- paste0(round(miho_demo_summary$mean_eng_read_j, 2), ' (', round(miho_demo_summary$sd_eng_read_j, 2), ')')
demo_table_j[8,2] <- paste0(round(miho_demo_summary$mean_eng_write_j, 2), ' (', round(miho_demo_summary$sd_eng_write_j, 2), ')')
demo_table_j <- as.data.frame(demo_table_j)

table101 <- flextable(demo_table_j) %>% set_caption('table101', caption=paste0('Table 101. Demographics for the Study 2 participants, N=', miho_demo_summary$N_total, ' (', miho_demo_summary$N_female_j, ' female)')) 
table101 <- autofit(table101)
table101

```

### Language History

### fTCD Data Quality and Reliability

```{r study2_data_quality, echo=FALSE, warning=FALSE, message=FALSE}
# Report number of excluded trials
excluded_trials_L1_Phon <- 100 - round(sum(L1_Phon_data$L1_Phon.N.N) / sum(L1_Phon_data$L1_Phon.nMark) * 100, 2)
excluded_trials_L2_Phon <- 100 - round(sum(L2_Phon_data$L2_Phon.N) / sum(L2_Phon_data$L2_Phon.nMark) * 100, 2)
excluded_trials_L1_Sem <- 100 - round(sum(L1_Sem_data$L1_Sem.N.N) / sum(L1_Sem_data$L1_Sem.nMark) * 100, 2)
excluded_trials_L2_Sem <- 100 - round(sum(L2_Sem_data$L2_Sem.N) / sum(L2_Sem_data$L2_Sem.nMark) * 100, 2)

# Here we will use the Shapiro Wilks normality test to see if the LI values for L1 and L2 are normally distributed
L1_Phon_normality <- shapiro.test(L1_Phon_data$L1_Phon.mean_LI)
L1_Sem_normality <- shapiro.test(L1_Sem_data$L1_Sem.mean_LI)
L2_Phon_normality <- shapiro.test(L2_Phon_data$L2_Phon.mean_LI)
L2_Sem_normality <- shapiro.test(L2_Sem_data$L2_Sem.mean_LI)

# spearman correlation test on odds and evens - means
L1_Phon_splithalf <- cor.test(L1_Phon_data$L1_Phon.mean_odd, L1_Phon_data$L1_Phon.mean_even, method = "spearman")
L2_Phon_splithalf <- cor.test(L2_Phon_data$L2_Phon.mean_odd, L2_Phon_data$L2_Phon.mean_even, method = "spearman")
L1_Sem_splithalf <- cor.test(L1_Sem_data$L1_Sem.mean_odd, L1_Sem_data$L1_Sem.mean_even, method = "spearman")
L2_Sem_splithalf <- cor.test(L2_Sem_data$L2_Sem.mean_odd, L2_Sem_data$L2_Sem.mean_even, method = "spearman")

```

### LI Values
```{r table4, echo=FALSE, warning=FALSE, message=FALSE}
table4 <- flextable(summary_stats) 
table4 <- autofit(table4)
set_caption(table4, caption='Table 4. Summary statistics for Study 2 laterality indices')

```

```{r Fig3_4, echo=FALSE, warning=FALSE, message=FALSE}
print(Fig3)
print(Fig4)

```

###  Correlations Between LI and Age of Acquisition

```{r study2_aoa, echo=FALSE, warning=FALSE, message=FALSE}

# Correlation between AOA and LIs
# normally distributed (lines 490-493)

total_info_j <- inner_join(x = plotdata_j, y = Miho_background_data, by = c("ID"))
aoa_l2_corr_phon_j <- cor.test(total_info_j$aoa_L2, total_info_j$L2_Phon.mean_LI, method = "pearson")
aoa_l2_corr_sem_j <- cor.test(total_info_j$aoa_L2, total_info_j$L2_Sem.mean_LI, method = "pearson")

```

## Discussion

# General Discussion


